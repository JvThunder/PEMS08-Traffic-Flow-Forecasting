{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Authors & References:","metadata":{}},{"cell_type":"markdown","source":"This notebook is created by:\n- Joshua Adrian Cahyono\n- Clayton Fernalo\n- Bryan Atista Kiely","metadata":{}},{"cell_type":"markdown","source":"### Dataset Reference:\n\nGuo, S., Lin, Y., Feng, N., Song, C., & Wan, H. (2019). Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01), 922-929. https://doi.org/10.1609/aaai.v33i01.3301922","metadata":{}},{"cell_type":"markdown","source":"### Additional Reference:\n\nChatGPT is used for debugging, clarification of concepts, and the writing of comments and descriptions. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"contents\"></a>\n# Table of Content\n1. [Problem Introduction](#section-1)\n2. [Import Libraries & Initialize Plots](#section-2)\n3. [Data Formatting](#section-3)\n4. [Data Visualization & EDA](#section-4)\n5. [Data Preparation](#section-5)\n6. [Train Model](#section-6)\n7. [Evaluate Prediction](#section-7)\n8. [Conclusion](#section-8)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-1\"></a>\n# Problem Introduction","metadata":{}},{"cell_type":"markdown","source":"We have found an interesting traffic dataset `PEMS-08 Dataset`, which contains the traffic data in San Bernardino from July to August in 2016. There are 170 locations with detectors recording every 5 minute intervals of traffic information. The dataset includes 3 features: `flow`, `occupy`, `speed`. The details of the features are as the following:\n\n- The flow variable in the PEMS08 dataset represents the number of vehicles that pass through the loop detector per time interval (5 minutes in this case). It is measured in vehicles per 5-minute interval.\n\n- The occupancy variable represents the proportion of time during the time interval (5 minutes) that the detector was occupied by a vehicle. It is measured as a percentage.\n\n- The speed variable represents the average speed of the vehicles passing through the loop detector during the time interval (5 minutes). It is measured in miles per hour (mph).\n \nFrom this, we thought of an interesting question: what if we create an model that can predict and properly rank the locations with the most traffic? We believe that this predictive task could potentially help the police in choosing the best locations to patrol given a particular time. To simply the problem, we believe that `occupy` encaptures the estimated traffic at a certain time, so we will be using `occupy` as our target variable.\n\nFormally, we formulated the following problem statement:\nFor $N$ prediction of the `occupy` variable and $K$ locations, make a predictive model to minimize $\\sum_{t=0}^{N-1} \\rho_t$, where $\\rho_t$ is defined as the [Spearman correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) of the prediction values and the actual values.\n\nThe Spearman correlation coefficient is a measure of the strength and direction of the monotonic relationship between two variables. It is not calculated based on the absolute value, but rather it only considers the relativity of the ranks of the variables. This is perfect for our task, since we want to focus on patrol location optimization.\n\nOur current strategy is to first focus on minimizing `RMSE` on predicting the actual values of the target variable `occupy`, and to simply use the rank of the values. For this, we will train an `LSTM` model and use the moving average of previous `occupy` values as the baseline.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-2\"></a>\n# Import Libraries & Initialize Plots","metadata":{}},{"cell_type":"markdown","source":"As always, we will first import all the libraries necessary for this notebook to run. We put all the imports at the top to make it easier to check the dependencies.","metadata":{}},{"cell_type":"code","source":"# Common libraries for data cleaning and visualization\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom numpy import load # use to load an npz file\nfrom scipy.signal import periodogram # use to graph a periodogram to get seasonality analysis\nfrom sklearn.preprocessing import MinMaxScaler # use to normalize the data features\n\n# keras library to create NN models\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\n\n# libraries for the metrics we will use (RMSE and Spearman)\nfrom keras.metrics import RootMeanSquaredError\nimport scipy.stats as stats","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:22:57.509258Z","iopub.execute_input":"2023-04-21T16:22:57.509540Z","iopub.status.idle":"2023-04-21T16:22:57.515849Z","shell.execute_reply.started":"2023-04-21T16:22:57.509513Z","shell.execute_reply":"2023-04-21T16:22:57.514649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In addition, we changed the notebook default plotting settings to make the graph prettier.","metadata":{}},{"cell_type":"code","source":"# Set plot settings\nplt.rcParams.update({'font.size': 12, 'font.family': 'serif'})\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['axes.grid'] = True\nplt.rcParams['axes.grid.which'] = 'both'\nplt.rcParams['grid.alpha'] = 0.5","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:22:57.529839Z","iopub.execute_input":"2023-04-21T16:22:57.532897Z","iopub.status.idle":"2023-04-21T16:22:57.544614Z","shell.execute_reply.started":"2023-04-21T16:22:57.532854Z","shell.execute_reply":"2023-04-21T16:22:57.541992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-3\"></a>\n# Data Formatting","metadata":{}},{"cell_type":"markdown","source":"Interestingly, the data is in the format of npz. npz is a file format used by the NumPy library to store arrays and metadata in a compressed format. We will first convert it to pandas dataframe, since we are more familiar with it.","metadata":{}},{"cell_type":"code","source":"data = load('/kaggle/input/traffic-flow/pems08.npz')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-21T16:22:57.549834Z","iopub.execute_input":"2023-04-21T16:22:57.552092Z","iopub.status.idle":"2023-04-21T16:22:57.609446Z","shell.execute_reply.started":"2023-04-21T16:22:57.552051Z","shell.execute_reply":"2023-04-21T16:22:57.608166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lst = data.files\nprint(data[lst[0]].shape)\nprint(data[lst[0]])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:22:57.611537Z","iopub.execute_input":"2023-04-21T16:22:57.612192Z","iopub.status.idle":"2023-04-21T16:22:58.575420Z","shell.execute_reply.started":"2023-04-21T16:22:57.612149Z","shell.execute_reply":"2023-04-21T16:22:58.574292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The file gives a numpy array with dimensions (17856, 170, 3), which are in the form of (timesteps, location, features). For the 3 features, the order is `flow`,`occupy`,`speed`.","metadata":{}},{"cell_type":"code","source":"traffic_data = data[lst[0]]\ndata_dict = []\n# loop for every timestep and every location and add as a single row\nfor timestep in range(traffic_data.shape[0]):\n    for location in range(traffic_data.shape[1]):\n        data_dict.append({\n            \"timestep\" : timestep+1,\n            \"location\" : location,\n            \"flow\"     : traffic_data[timestep][location][0],\n            \"occupy\"   : traffic_data[timestep][location][1],\n            \"speed\"    : traffic_data[timestep][location][2]\n        })","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:22:58.577056Z","iopub.execute_input":"2023-04-21T16:22:58.577727Z","iopub.status.idle":"2023-04-21T16:23:05.363293Z","shell.execute_reply.started":"2023-04-21T16:22:58.577684Z","shell.execute_reply":"2023-04-21T16:23:05.361998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We changed it into a list of dictionary. Each row represents a single timestep in a single location.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data_dict)\ndf.to_csv(\"traffic.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:05.367779Z","iopub.execute_input":"2023-04-21T16:23:05.368859Z","iopub.status.idle":"2023-04-21T16:23:17.233203Z","shell.execute_reply.started":"2023-04-21T16:23:05.368814Z","shell.execute_reply":"2023-04-21T16:23:17.231720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we save it into a csv file called `traffic.csv`. So, if you already have the file `traffic.csv`, you won't need to run the code above.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-4\"></a>\n# Data Visualization & EDA","metadata":{}},{"cell_type":"markdown","source":"Now it is time for Data Visualization and Exploratory Data Analysis. Here, we will see through the data and clean and apply feature engineering when necessary.","metadata":{}},{"cell_type":"code","source":"traffic = pd.read_csv(\"/kaggle/working/traffic.csv\")\nprint(len(traffic))\ntraffic.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:17.237563Z","iopub.execute_input":"2023-04-21T16:23:17.237955Z","iopub.status.idle":"2023-04-21T16:23:18.093748Z","shell.execute_reply.started":"2023-04-21T16:23:17.237922Z","shell.execute_reply":"2023-04-21T16:23:18.092578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data consist of $17856 \\cdot 170 = 3035520$ rows. That is a lot of rows! We will have to handle that later, but for now, let us look into the data.","metadata":{}},{"cell_type":"code","source":"traffic.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:18.095309Z","iopub.execute_input":"2023-04-21T16:23:18.095970Z","iopub.status.idle":"2023-04-21T16:23:18.114365Z","shell.execute_reply.started":"2023-04-21T16:23:18.095929Z","shell.execute_reply":"2023-04-21T16:23:18.113304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the data seem to be either integer or float, nothing out of the ordinarily here. `flow`, `occupy` and `speed` are numerical and continuous data.","metadata":{}},{"cell_type":"code","source":"traffic.count()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:18.115819Z","iopub.execute_input":"2023-04-21T16:23:18.116249Z","iopub.status.idle":"2023-04-21T16:23:18.154021Z","shell.execute_reply.started":"2023-04-21T16:23:18.116213Z","shell.execute_reply":"2023-04-21T16:23:18.152882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:18.155578Z","iopub.execute_input":"2023-04-21T16:23:18.156018Z","iopub.status.idle":"2023-04-21T16:23:18.186510Z","shell.execute_reply.started":"2023-04-21T16:23:18.155981Z","shell.execute_reply":"2023-04-21T16:23:18.185423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also nothing surprising here, and fortunately the data has no null values, so we don't need to impute any values.","metadata":{}},{"cell_type":"code","source":"traffic.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:18.188260Z","iopub.execute_input":"2023-04-21T16:23:18.188610Z","iopub.status.idle":"2023-04-21T16:23:18.699447Z","shell.execute_reply.started":"2023-04-21T16:23:18.188574Z","shell.execute_reply":"2023-04-21T16:23:18.698328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic['flow'].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:18.703952Z","iopub.execute_input":"2023-04-21T16:23:18.704248Z","iopub.status.idle":"2023-04-21T16:23:19.175588Z","shell.execute_reply.started":"2023-04-21T16:23:18.704220Z","shell.execute_reply":"2023-04-21T16:23:19.174571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic['occupy'].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:19.178233Z","iopub.execute_input":"2023-04-21T16:23:19.178899Z","iopub.status.idle":"2023-04-21T16:23:19.685116Z","shell.execute_reply.started":"2023-04-21T16:23:19.178856Z","shell.execute_reply":"2023-04-21T16:23:19.683872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traffic['speed'].hist(bins=100);","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:19.689482Z","iopub.execute_input":"2023-04-21T16:23:19.689834Z","iopub.status.idle":"2023-04-21T16:23:20.157882Z","shell.execute_reply.started":"2023-04-21T16:23:19.689805Z","shell.execute_reply":"2023-04-21T16:23:20.156913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the distribution of `flow`, `occupy` and `speed`, it looks like they are spread out nicely and roughly follows a nice (a bit skewed) normal distribution. However, one thing to note is that the order of magnitude of the values are quite different, so we should normalize and scale the data later.","metadata":{}},{"cell_type":"markdown","source":"Now, we would like to explore the data further. For this, we will choose a single random location.","metadata":{}},{"cell_type":"code","source":"location_0 = traffic[traffic[\"location\"]==50].reset_index()\nlocation_0.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:20.159478Z","iopub.execute_input":"2023-04-21T16:23:20.159875Z","iopub.status.idle":"2023-04-21T16:23:20.180743Z","shell.execute_reply.started":"2023-04-21T16:23:20.159836Z","shell.execute_reply":"2023-04-21T16:23:20.179623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_0[\"occupy\"][:1000].plot()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:20.182377Z","iopub.execute_input":"2023-04-21T16:23:20.182755Z","iopub.status.idle":"2023-04-21T16:23:20.429892Z","shell.execute_reply.started":"2023-04-21T16:23:20.182718Z","shell.execute_reply":"2023-04-21T16:23:20.428837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_0[\"flow\"][:1000].plot()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:20.431366Z","iopub.execute_input":"2023-04-21T16:23:20.432020Z","iopub.status.idle":"2023-04-21T16:23:20.666216Z","shell.execute_reply.started":"2023-04-21T16:23:20.431981Z","shell.execute_reply":"2023-04-21T16:23:20.665148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_0[\"speed\"][:1000].plot();","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:20.667574Z","iopub.execute_input":"2023-04-21T16:23:20.668540Z","iopub.status.idle":"2023-04-21T16:23:20.896596Z","shell.execute_reply.started":"2023-04-21T16:23:20.668497Z","shell.execute_reply":"2023-04-21T16:23:20.895600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We limit the plot to the first 1000 values since there are too many timesteps and the plot will be hard to look and analyze. From the plot above (and checking some other random locations), there seem to be a strong seasonality on `flow` and more importantly `occupy` , which is out target variable. ","metadata":{}},{"cell_type":"markdown","source":"Of course in any time series prediction, lag features are used as the main features. Specifically, $k$ lag features means that we use the last $k$ values to predict the next value. So, it is a good measure to check the correlation of present $(t_n)$ and future $(t_{n+1})$ values","metadata":{}},{"cell_type":"code","source":"COR_STEP = 1\npres = traffic[['flow', 'occupy', 'speed']][0:-(COR_STEP)].reset_index(drop=True)\nfuture = traffic[['flow', 'occupy', 'speed']][COR_STEP:] \\\n    .reset_index(drop=True) \\\n    .add_suffix('_future')\nval = pres.join(future)\nval.corr()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:20.898672Z","iopub.execute_input":"2023-04-21T16:23:20.899322Z","iopub.status.idle":"2023-04-21T16:23:21.591665Z","shell.execute_reply.started":"2023-04-21T16:23:20.899281Z","shell.execute_reply":"2023-04-21T16:23:21.590626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this (and some other locations we check), we can see that the correlation of all the variables to occupy is moderately high. (It might not look that high, but considering that we can use multiple steps of lag features, it is a pretty good correlation.) In addition, it is also nice to see that the correlation of `flow` and `speed` to `occupy` is also high, which suggests that the variables might be linearly correlated and could be used to predict each other.","metadata":{}},{"cell_type":"code","source":"def plot_periodogram(ts, detrend='linear', ax=None):\n    \"\"\"\n    Plots the periodogram of a time series.\n\n    Args:\n        ts (pd.Series): A time series.\n        detrend (str): Detrending method for the time series.\n        ax (matplotlib.axes.Axes): The axes on which to plot.\n\n    Returns:\n        ax (matplotlib.axes.Axes): The axes on which the periodogram is plotted.\n    \"\"\"\n    fs = pd.Timedelta(weeks=4) / pd.Timedelta(minutes=5)\n    frequencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n        \n    ax.step(frequencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([4, 30, 30*24])\n    ax.set_xticklabels(\n        [\n            \"Weekly\",\n            \"Daily\",\n            \"Hourly\"\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax\n\nplot_periodogram(location_0[\"occupy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:21.593043Z","iopub.execute_input":"2023-04-21T16:23:21.593703Z","iopub.status.idle":"2023-04-21T16:23:22.071355Z","shell.execute_reply.started":"2023-04-21T16:23:21.593662Z","shell.execute_reply":"2023-04-21T16:23:22.068993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we graph a [periodogram](https://en.wikipedia.org/wiki/Periodogram) to test the seasonality of the `occupy` variable. The periodogram shows the strength of the frequencies present in the time series, and can be used to identify the dominant frequencies or periodicities in the data. From the graph above, it suggests that there seem to be a pattern in the occupy variable daily, which makes sense, since many people work and travel on a fixed schedule. (for example busy hours after work will have higher traffic) For this reason, we will add an `hour` feature on prediction.","metadata":{}},{"cell_type":"markdown","source":"As said earlier, the number of rows in this is pretty large (and when we tried add >10 lag features, the memory limit exceeded >8gb). So, we will instead be working on hourly timesteps instead of 5 minutes interval. The data within an hour (12 5-minute timesteps) will be averaged.","metadata":{}},{"cell_type":"code","source":"location_0[\"hour\"] = ((location_0[\"timestep\"] - 1) // 12)\ngrouped = location_0.groupby(\"hour\").mean().reset_index()\ngrouped.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:22.072683Z","iopub.execute_input":"2023-04-21T16:23:22.073532Z","iopub.status.idle":"2023-04-21T16:23:22.095670Z","shell.execute_reply.started":"2023-04-21T16:23:22.073481Z","shell.execute_reply":"2023-04-21T16:23:22.094678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped[\"occupy\"][:24*5].plot();","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:22.096990Z","iopub.execute_input":"2023-04-21T16:23:22.097420Z","iopub.status.idle":"2023-04-21T16:23:22.333551Z","shell.execute_reply.started":"2023-04-21T16:23:22.097379Z","shell.execute_reply":"2023-04-21T16:23:22.332572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COR_STEP = 12\npres = grouped[['flow', 'occupy', 'speed']][0:-(COR_STEP)].reset_index(drop=True)\nfuture = grouped[['flow', 'occupy', 'speed']][COR_STEP:] \\\n    .reset_index(drop=True) \\\n    .add_suffix('_future')\nval = pres.join(future)\nval.corr()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:22.335310Z","iopub.execute_input":"2023-04-21T16:23:22.335707Z","iopub.status.idle":"2023-04-21T16:23:22.356830Z","shell.execute_reply.started":"2023-04-21T16:23:22.335668Z","shell.execute_reply":"2023-04-21T16:23:22.355908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After merging the timesteps, we get a higher correlation with future values, which is nice.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-5\"></a>\n# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"Now that we are done with the EDA, it is time to prepare our data for prediction. We add the features we found useful from the EDA. We also normalize and split our data to train and test datasets.","metadata":{}},{"cell_type":"code","source":"# creating 3-dimensional array for [timestep, timeframe, features]\ndef create_dataset(location, WINDOW_SIZE):\n    \n    # mask a certain location\n    location_current = traffic[traffic[\"location\"]==location].reset_index()\n    \n    # group to hour and average 12 (5-minute) timesteps\n    location_current[\"hour\"] = ((location_current[\"timestep\"] - 1) // 12)\n    grouped = location_current.groupby(\"hour\").mean().reset_index()\n    \n    # add hour features as mod 24 cycle (0...23)\n    grouped['day'] = (grouped['hour'] // 24) % 7\n    grouped['hour'] %= 24\n    \n    one_hot_hour = pd.get_dummies(grouped['hour'])\n    one_hot_hour = one_hot_hour.add_prefix('hour_')\n    \n    # merge all the features together to get a total of 27 features\n    hour_grouped = pd.concat([grouped[[\"occupy\", \"flow\", \"speed\"]], one_hot_hour], axis=1)\n    hour_grouped = np.array(hour_grouped)\n    \n    X, Y = [], []\n    \n    # add lag features (in reverse time order)\n    for i in range(len(hour_grouped) - WINDOW_SIZE):\n        X.append(hour_grouped[i:(i + WINDOW_SIZE)][::-1]) # reverse the order\n        Y.append(hour_grouped[i + WINDOW_SIZE, 0]) # index 0 is occupy\n    \n    return X,Y # returns (timestep, timeframe, features) and (target)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:22.358094Z","iopub.execute_input":"2023-04-21T16:23:22.358907Z","iopub.status.idle":"2023-04-21T16:23:22.368142Z","shell.execute_reply.started":"2023-04-21T16:23:22.358869Z","shell.execute_reply":"2023-04-21T16:23:22.367033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make a `create_dataset` function to make all the features that we need for the prediction (hour features and lag features as multiple dimensions). Each lag step (lag steps total of WINDOW_SIZE) will be included to X in the second dimension (in reverse order). We format the array X as dimension (timestep, timeframe, features) and Y as dimension (target).","metadata":{}},{"cell_type":"code","source":"# creating 4-th dimension for the locations\nX, Y = [], []\n\nfor location in range(170):\n    a,b = create_dataset(location, WINDOW_SIZE=24)\n    X.append(a)\n    Y.append(b)\n    \nX = np.moveaxis(X,0,-1)\nY = np.moveaxis(Y,0,-1)\n\nprint(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:22.369496Z","iopub.execute_input":"2023-04-21T16:23:22.369899Z","iopub.status.idle":"2023-04-21T16:23:25.020424Z","shell.execute_reply.started":"2023-04-21T16:23:22.369862Z","shell.execute_reply":"2023-04-21T16:23:25.019251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally here, we merge all the locations together to get X `(timestep, timeframe, features, location)` and Y `(timestep, location)`.","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nTEST_SIZE  = 0.2\n\ntrain_size = int(len(X) * TRAIN_SIZE)\ntest_size  = int(len(X) * TEST_SIZE)\n\ntrain_X, train_Y = X[:train_size], Y[:train_size]\ntest_X, test_Y = X[train_size:], Y[train_size:]\n\nprint(train_X.shape)\nprint(train_Y.shape)\nprint(test_X.shape)\nprint(test_Y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:25.021904Z","iopub.execute_input":"2023-04-21T16:23:25.022354Z","iopub.status.idle":"2023-04-21T16:23:25.030276Z","shell.execute_reply.started":"2023-04-21T16:23:25.022314Z","shell.execute_reply":"2023-04-21T16:23:25.028990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we split the data into train and test with `train : test` ratio of `0.8 : 0.2`. In a time series analysis, we don't usually use random splitting because it wont make sense to predict data in gaps, so we split the first 80% as train while the last 20% is used as test set. ","metadata":{}},{"cell_type":"code","source":"scaler_X = MinMaxScaler()\nscaler_Y = MinMaxScaler()\ntrain_X = scaler_X.fit_transform(train_X.reshape(train_X.shape[0] * train_X.shape[1], -1)) \\\n                   .reshape(train_X.shape[0], train_X.shape[1], -1)\ntest_X = scaler_X.transform(test_X.reshape(test_X.shape[0] * test_X.shape[1], -1)) \\\n                   .reshape(test_X.shape[0], test_X.shape[1], -1)\ntrain_Y = scaler_Y.fit_transform(train_Y)\ntest_Y = scaler_Y.transform(test_Y)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:25.031778Z","iopub.execute_input":"2023-04-21T16:23:25.032443Z","iopub.status.idle":"2023-04-21T16:23:26.879655Z","shell.execute_reply.started":"2023-04-21T16:23:25.032401Z","shell.execute_reply":"2023-04-21T16:23:26.878576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we apply a `MinMaxScaler` to scale both X and Y features to the range (0,1). (Note that we need to use a different scaler for X and Y. Also, the good practice is to not fit using test since we assume that it is a fresh data and we have no information about it, so we only using train split to fit the scaler.","metadata":{}},{"cell_type":"code","source":"print(train_X.shape)\nprint(test_X.shape)\nprint(train_Y.shape)\nprint(test_Y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:26.881414Z","iopub.execute_input":"2023-04-21T16:23:26.884763Z","iopub.status.idle":"2023-04-21T16:23:26.890023Z","shell.execute_reply.started":"2023-04-21T16:23:26.884731Z","shell.execute_reply":"2023-04-21T16:23:26.888859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the shape changed a bit for X. I reshaped it so that the dimensions are `(timestep, timeframe, features*location)`. (we merged the locations together so that the final dimensions have features for every 170 locations.)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-6\"></a>\n# Model Training","metadata":{}},{"cell_type":"markdown","source":"Finally, the exciting part! We get to see our prediction model in action. We will use an LSTM layer as the input layer while we will output a vector of 170 values, each one to predict the value of `occupy` of each location. \n\nThe reason we used an LSTM layer is because the LSTM is built to remember information from earlier timesteps and gain information from their relation.\n\nWe will use `MSE` for the loss and `RMSE` as the metric. (metric will not be used for backpropagation and only serve to look at how well the prediction is at that iteration)\n\nAlso, we use `ReLu` since our task is a regression task.","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    LSTM(256, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])),\n    LSTM(256, return_sequences=False),\n    Dropout(0.2),\n    Dense(256, activation='relu'),\n    Dropout(0.2),\n    Dense(170, activation='linear'),\n])\n\nmodel.compile(loss='mse', optimizer='adam', metrics=[RootMeanSquaredError()])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:26.891608Z","iopub.execute_input":"2023-04-21T16:23:26.892185Z","iopub.status.idle":"2023-04-21T16:23:27.414517Z","shell.execute_reply.started":"2023-04-21T16:23:26.892148Z","shell.execute_reply":"2023-04-21T16:23:27.413519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:27.420924Z","iopub.execute_input":"2023-04-21T16:23:27.421230Z","iopub.status.idle":"2023-04-21T16:23:27.444181Z","shell.execute_reply.started":"2023-04-21T16:23:27.421201Z","shell.execute_reply":"2023-04-21T16:23:27.443422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:27.445156Z","iopub.execute_input":"2023-04-21T16:23:27.445476Z","iopub.status.idle":"2023-04-21T16:23:27.747860Z","shell.execute_reply.started":"2023-04-21T16:23:27.445441Z","shell.execute_reply":"2023-04-21T16:23:27.746613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the model architecture is set, it is time to train the model. We can tune the hyperparemeters: epoch (the number of training iterations) and batch_size (the number of rows to include in a single forward and backpropagation). We will include validation split of 0.1 (by default it will take 10% of the last few rows and use it as validation data, which is not used for training but we can use it to evaluate on an unseen data for every given step).","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_X, train_Y, epochs=150, batch_size=32, validation_split=0.1, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:23:27.750197Z","iopub.execute_input":"2023-04-21T16:23:27.750655Z","iopub.status.idle":"2023-04-21T16:25:54.337590Z","shell.execute_reply.started":"2023-04-21T16:23:27.750580Z","shell.execute_reply":"2023-04-21T16:25:54.336475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After fitting our training set with the model, we can evaluate whether our model overfits the training set or not. One way of doing it is by plotting the loss functions at each step of the training.","metadata":{}},{"cell_type":"code","source":"def plot_training(training_history, text, width):\n    history = training_history.history[text]\n    \n    # creates a moving average plot to reduce variations\n    moving_average = [float(\"NaN\") for i in range(width)]\n    for i in range(width, len(history)+1):\n        moving_average.append(np.mean(np.array(history[i-width:i+1])))\n        \n    plt.plot(history)\n    plt.plot(moving_average)\n    plt.title(text)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['value','moving average'], loc='upper left')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:54.339757Z","iopub.execute_input":"2023-04-21T16:25:54.340254Z","iopub.status.idle":"2023-04-21T16:25:54.347696Z","shell.execute_reply.started":"2023-04-21T16:25:54.340209Z","shell.execute_reply":"2023-04-21T16:25:54.346659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plots the loss (MSE and RMSE) of both test and validation data\nWIDTH = 10\nplot_training(history,'loss',WIDTH)\nplot_training(history,'val_loss',WIDTH)\nplot_training(history,'val_root_mean_squared_error',WIDTH)\nplot_training(history,'root_mean_squared_error',WIDTH)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:54.349274Z","iopub.execute_input":"2023-04-21T16:25:54.350356Z","iopub.status.idle":"2023-04-21T16:25:55.319119Z","shell.execute_reply.started":"2023-04-21T16:25:54.350318Z","shell.execute_reply":"2023-04-21T16:25:55.318160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe that the validation loss value flattening after 120 epochs of training, despite that the training loss keep decreasing. If the training keeps going, our model would overfit to the training set (lower training loss but higher validation loss). In order to avoid those issues, it is best that we would limit the number of training epochs to around that value, which is 150 epochs for this case.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-7\"></a>\n# Evaluate Prediction","metadata":{}},{"cell_type":"markdown","source":"Now time to evaluate how good our model is. We add a moving average as the baseline. The reason we need a baseline is to see whether our RMSE score and Spearman score is actually good, not just due to random guessing.","metadata":{}},{"cell_type":"code","source":"def predict_and_calc_score(X, Y):\n    # prediction of Y using the model\n    pred = model.predict(X) \n    # moving average of Y\n    window_size = 12\n    moving_average = np.apply_along_axis(\n        lambda x: np.convolve(x, np.ones(window_size)/window_size, mode='same'), axis=0, arr=Y)\n    moving_average = np.concatenate((np.zeros((1, Y.shape[1])), moving_average), axis=0)[:Y.shape[0]]\n    \n    # scale it back to the original scale\n    pred_scaled = scaler_Y.inverse_transform(pred)\n    moving_average_scaled = scaler_Y.inverse_transform(moving_average)\n    Y_scaled = scaler_Y.inverse_transform(Y)\n    \n    # calculate the RMSE\n    baseline_RMSE = np.sqrt(np.mean((Y_scaled - moving_average_scaled) ** 2))\n    model_RMSE = np.sqrt(np.mean((Y_scaled - pred_scaled) ** 2))\n    \n    return Y_scaled, pred_scaled, moving_average_scaled, model_RMSE, baseline_RMSE\n\ndef plot_prediction(actual, prediction, moving_average):\n    # Plot the actual values\n    plt.plot(actual, label=\"True value\", linestyle='-', linewidth=1, marker='s', markersize=1)\n    \n    # Plot the moving average\n    plt.plot(moving_average, label=\"Moving Average\", linestyle='--', linewidth=1, marker='s', markersize=1)\n    \n    # Plot the predicted values\n    plt.plot(prediction, label=\"Prediction\", linestyle='--', linewidth=1, marker='o', markersize=1)\n    \n    # Set the title and axis labels\n    plt.title('Prediction vs. True Value', fontsize=16)\n    plt.xlabel('Hour Timesteps')\n    plt.ylabel('Output Value')\n\n    # Add a legend to the plot\n    plt.legend(loc='upper left', fontsize=12)\n    \n    # Display the plot\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:55.320741Z","iopub.execute_input":"2023-04-21T16:25:55.321173Z","iopub.status.idle":"2023-04-21T16:25:55.331942Z","shell.execute_reply.started":"2023-04-21T16:25:55.321123Z","shell.execute_reply":"2023-04-21T16:25:55.330703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_actual, train_prediction, train_moving_average, train_RMSE, baseline_RMSE = \\\n    predict_and_calc_score(train_X, train_Y)\n\nprint(\"Train Moving Average RMSE:\", baseline_RMSE)\nprint(\"Train Prediction RMSE:\", train_RMSE)\n\ncorr, pval = stats.spearmanr(train_actual, train_moving_average)\nmov_spearman_corr = np.mean(corr)\ncorr, pval = stats.spearmanr(train_actual, train_prediction)\npred_spearman_corr = np.mean(corr)\n\nprint(\"Train Moving Average Spearman Correlation:\", mov_spearman_corr)\nprint(\"Train Prediction Spearman Correlation:\", pred_spearman_corr)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:55.333416Z","iopub.execute_input":"2023-04-21T16:25:55.334455Z","iopub.status.idle":"2023-04-21T16:25:57.989006Z","shell.execute_reply.started":"2023-04-21T16:25:55.334405Z","shell.execute_reply":"2023-04-21T16:25:57.987557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location = 3\nplot_prediction(train_actual[:,location], train_prediction[:,location], train_moving_average[:,location])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:57.994907Z","iopub.execute_input":"2023-04-21T16:25:57.995420Z","iopub.status.idle":"2023-04-21T16:25:58.297548Z","shell.execute_reply.started":"2023-04-21T16:25:57.995369Z","shell.execute_reply":"2023-04-21T16:25:58.296554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train RMSE and Spearman is good. It is better than the moving average and shows that the model is not underfitting. However, the true test lies on the test evaluation (since Neural Network models might overfit).","metadata":{}},{"cell_type":"code","source":"test_actual, test_prediction, test_moving_average, test_RMSE, baseline_RMSE = \\\n    predict_and_calc_score(test_X, test_Y)\n\nprint(\"Test Moving Average RMSE:\", baseline_RMSE)\nprint(\"Test Prediction RMSE:\", test_RMSE)\n\ncorr, pval = stats.spearmanr(test_actual, test_moving_average)\nmov_spearman_corr = np.mean(corr)\ncorr, pval = stats.spearmanr(test_actual, test_prediction)\npred_spearman_corr = np.mean(corr)\n\nprint(\"Test Moving Average Spearman Correlation:\", mov_spearman_corr)\nprint(\"Test Prediction Spearman Correlation:\", pred_spearman_corr)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:58.299294Z","iopub.execute_input":"2023-04-21T16:25:58.300054Z","iopub.status.idle":"2023-04-21T16:25:59.023568Z","shell.execute_reply.started":"2023-04-21T16:25:58.300015Z","shell.execute_reply":"2023-04-21T16:25:59.020609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location = 10\nplot_prediction(test_actual[:,location], test_prediction[:,location], test_moving_average[:,location])","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:59.025094Z","iopub.execute_input":"2023-04-21T16:25:59.025756Z","iopub.status.idle":"2023-04-21T16:25:59.330088Z","shell.execute_reply.started":"2023-04-21T16:25:59.025713Z","shell.execute_reply":"2023-04-21T16:25:59.329154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test RMSE and Spearman is also good! It is better than the moving average and shows that the model is not overfitting and can generalize well to new data.","metadata":{}},{"cell_type":"code","source":"model.save('LSTM_model')","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:25:59.331651Z","iopub.execute_input":"2023-04-21T16:25:59.332222Z","iopub.status.idle":"2023-04-21T16:26:07.346315Z","shell.execute_reply.started":"2023-04-21T16:25:59.332181Z","shell.execute_reply":"2023-04-21T16:26:07.345202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r LSTM_Model.zip /kaggle/working/LSTM_model","metadata":{"execution":{"iopub.status.busy":"2023-04-21T16:26:07.348447Z","iopub.execute_input":"2023-04-21T16:26:07.349919Z","iopub.status.idle":"2023-04-21T16:26:10.417778Z","shell.execute_reply.started":"2023-04-21T16:26:07.349875Z","shell.execute_reply":"2023-04-21T16:26:10.416582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can also save the model.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-8\"></a>\n# Conclusion","metadata":{}},{"cell_type":"markdown","source":"To conclude, we have made a predictive model using an LSTM neural network. We included hour and lag features, and also carefully scale and split the data. In the end, our model was able to predict both the train and test data better than our baseline which is moving average of previous values. We believe that this model can be further optimized and tested to help solve one of the real world issues, which is traffic management.","metadata":{}}]}